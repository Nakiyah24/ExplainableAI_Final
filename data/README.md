# Healthcare Prediction Model with Explainable AI

This project predicts hospitalization risk using the MEPS (Medical Expenditure Panel Survey) dataset. I built an XGBoost and LightGBM model and found LightGBM to be slightly superior. Thus, I retrained the data on my full training dataset and exported all relevant data and models for it to be used in the front end. I then created the interactive Streamlit app to explore predictions and understand what drives them. The main focus is on explainability - making it clear why the model makes certain predictions and how different factors affect risk.

## What This Project Does

The system predicts whether an individual is likely to experience a hospital stay based on their:
- demographic characteristics
- socioeconomic factors
- health conditions

The final LightGBM model provides high predictive performance, while SHAP values offer a clear, intuitive view of why the model makes each prediction. Users can see which features increase or decrease hospitalization risk, compare outcomes across demographic groups, and explore fairness-related patterns in model behavior.

### Files

1. **`model.ipynb`** - My main notebook where I did all the data cleaning, model training, and analysis
2. **`app.py`** - The Streamlit web app I built to interact with the model (see acknowledgments below)

## Dataset

I used the **MEPS 2023 Full Year Consolidated Data File (PUF Number: HC251)** from the Medical Expenditure Panel Survey. MEPS is a great dataset for healthcare research because it tracks people's medical expenses and health status over time.

- **Official Source**: [MEPS Website](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files.jsp)

The data includes:
- Demographics: age, sex, race/ethnicity
- Socioeconomic stuff: income, education, poverty level, region
- Health status: chronic conditions (hypertension, diabetes, asthma), smoking status
- Insurance coverage
- Healthcare utilization: inpatient expenditures (I used this to create the target variable - whether someone was hospitalized)

## Installation

You'll need Python 3.8 or higher. Then just:

1. Clone or download this repo

2. Install the packages:
```bash
pip install -r requirements.txt
```

Main packages:
- `streamlit` - For the web app
- `pandas`, `numpy`, `polars` - Data processing (polars is fast!)
- `lightgbm` - The model I used
- `shap` - For explainability
- `scikit-learn` - ML utilities
- `plotly`, `matplotlib` - Plotting
- `joblib` - Saving/loading models
- `openpyxl` - Reading Excel files

## Usage

### Running the Analysis Notebook

1. Make sure the Excel file `h251.xlsx` is in the `data/meps_2023/` directory

2. Open `model.ipynb` in Jupyter Notebook or JupyterLab

3. The notebook walks through:
   - Loading and cleaning the MEPS data (handling those weird negative codes they use for missing values)
   - Feature engineering - renaming variables, one-hot encoding, creating the target variable
   - Model training - I compared LightGBM and XGBoost, LightGBM won
   - Evaluation - ROC-AUC, precision, recall, all that good stuff
   - Explainability - SHAP values to understand what the model is doing
   - Exporting everything needed for the Streamlit app

4. Run all cells to train the model and generate all the artifacts

**Note**: The notebook expects the data file at `./data/meps_2023/h251.xlsx`. Make sure this file exists before running the notebook.

### Running the Streamlit App

Just run:
```bash
streamlit run app.py
```

It'll open in your browser automatically.

## App Features

The app lets you play around with the model interactively:

### Step 1: Model Prediction
Use the sidebar to set patient characteristics, then see:
- Predicted hospitalization risk as a percentage
- Risk category (Low/Medium/High)
- Binary prediction (Hospitalized/Not Hospitalized) based on a threshold you can adjust

### Step 2: Local SHAP Explanation
- **SHAP Waterfall Plot**: Shows how each feature pushes the prediction up or down
- **Top Features**: The 10 most important features for this specific prediction
- **Summary**: A quick text explanation of what's driving the risk

### Step 3: What If? Analysis
Change features like insurance type, age, or poverty category and see how the prediction changes in real-time. Useful for understanding how different factors affect risk.

### Step 4: ICE Curve - Age Effect
An interactive plot showing how risk changes as age increases (keeping everything else constant). There's a line marking the current patient's age so you can see where they fall on the curve.

## File Structure

```
.
├── model.ipynb          # Main analysis notebook
├── app.py                     # Streamlit web application
├── requirements.txt           # Python dependencies
├── README.md                  # This file
│
├── fairness_artifacts/        # Model and training data (generated by model.ipynb)
│   ├── final_lightgbm_model.pkl
│   ├── X_train.parquet
│   ├── X_test.parquet
│   ├── y_train.parquet
│   └── y_test.parquet
│
└── fairness_results/          # Pre-computed results (generated by model.ipynb)
    ├── shap_explainer.pkl
    └── df_fair_with_shap.parquet
```

## Model Details

### Algorithm
I used **LightGBM** after comparing it with XGBoost. LightGBM performed slightly better (ROC-AUC ~0.74 vs 0.72) and trains faster. I used stratified 5-fold cross-validation and made sure each fold had a good balance of hospitalized vs non-hospitalized cases.

### Target Variable
I created a binary `hospitalized` variable: 1 if someone had any inpatient expenditures > $0, 0 otherwise. This comes from the `inpatient_expenditures` field in the MEPS data.

### Features
The model uses about 50 features:
- **Numeric**: age, education_years, family_income, years_in_us
- **Categorical** (one-hot encoded): sex, race/ethnicity, poverty category, insurance coverage, region, chronic conditions, smoking status

## Explainability

I used **SHAP (SHapley Additive exPlanations)** to understand what the model is doing:

- **Global Importance**: Which features matter most overall (age is by far the biggest predictor)
- **Local Explanations**: For each individual prediction, which features pushed it up or down
- **Dependence Plots**: How specific features affect predictions across their value ranges

## Technical Notes

- The app uses Streamlit's caching to avoid reloading the model every time
- Feature vectors have to match the exact order the model expects
- SHAP values are computed on-the-fly for real-time explanations
- Missing values are handled with median imputation for numeric features

## Acknowledgments

**Important Note**: I used ChatGPT-5 to help me build the Streamlit app code (`app.py`). The app structure, UI components, and Streamlit-specific code were developed partly with llm assistance. However, **all of the analysis, model training, feature engineering, interpretations, and insights in the notebook are entirely my own work**. The model choices, evaluation approach, SHAP analysis, and all conclusions are all my own based on what we have learned in class and the assignments we have done for the class.

## References

- **MEPS Dataset**: [AHRQ MEPS Website](https://meps.ahrq.gov/mepsweb/)
- **Model Approach**: Based on literature research 
- **SHAP Documentation**: [SHAP GitHub](https://github.com/slundberg/shap)

## License

This project is for educational/research purposes.
