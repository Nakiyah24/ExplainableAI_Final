{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6356aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import random\n",
    "import joblib\n",
    "import pathlib\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "# Display options (optional)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d2337",
   "metadata": {},
   "source": [
    "## Step 1 — Load artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20adfed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load path and get all the required files\n",
    "def get_paths(base=\"fairness_artifacts\"):\n",
    "    base = pathlib.Path(base)\n",
    "    return {\n",
    "        \"X_train\": base / \"X_train.parquet\",\n",
    "        \"X_test\": base / \"X_test.parquet\",\n",
    "        \"y_train\": base / \"y_train.parquet\",\n",
    "        \"y_test\": base / \"y_test.parquet\",\n",
    "        \"df_meta\": base / \"df_meta.parquet\",\n",
    "        \"ids_test\": base / \"ids_test.parquet\",\n",
    "        \"model\": base / \"final_lightgbm_model.pkl\",\n",
    "    }\n",
    "\n",
    "\n",
    "def load_artifacts(base=\"fairness_artifacts\"):\n",
    "    paths = get_paths(base)\n",
    "\n",
    "    X_train = pd.read_parquet(paths[\"X_train\"])\n",
    "    X_test = pd.read_parquet(paths[\"X_test\"])\n",
    "    y_train = pd.read_parquet(paths[\"y_train\"])[\"hospitalized\"]\n",
    "    y_test = pd.read_parquet(paths[\"y_test\"])[\"hospitalized\"]\n",
    "    df_meta = pd.read_parquet(paths[\"df_meta\"])\n",
    "    ids_test = pd.read_parquet(paths[\"ids_test\"])[\"person_id\"]\n",
    "    model = joblib.load(paths[\"model\"])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, df_meta, ids_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d3f9b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, df_meta, ids_test, final_lgb = load_artifacts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd50b35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      "X Train: (15135, 65)\n",
      "X Test: (3784, 65)\n",
      "y Train: (15135,)\n",
      "y Test: (3784,)\n",
      "ids_test: (3784,)\n",
      "df_meta: (18919, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\")\n",
    "print(\"X Train:\", X_train.shape)\n",
    "print(\"X Test:\", X_test.shape)\n",
    "print(\"y Train:\", y_train.shape)\n",
    "print(\"y Test:\", y_test.shape)\n",
    "print(\"ids_test:\", ids_test.shape)\n",
    "print(\"df_meta:\", df_meta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325b6cb",
   "metadata": {},
   "source": [
    "## Step 2 — Build base evaluation table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4041344c",
   "metadata": {},
   "source": [
    "For the fairness playground, I need a clean table that captures what the model actually did on the test set. Thus, I will now build a base evaluation table on the test set that includes person_id, the true label, the predicted probability, and the final 0/1 prediction. This is the core model-output layer I’ll use when joining with demographics and computing group-wise fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "892d6850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_base_eval_table(X_test, y_test, ids_test, model, threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    Build a base evaluation table on the test set.\n",
    "    \"\"\"\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (proba >= threshold).astype(int)\n",
    "\n",
    "    df_eval = pd.DataFrame(\n",
    "        {\n",
    "            \"person_id\": ids_test.values,\n",
    "            \"y_true\": y_test.values,\n",
    "            \"y_pred_proba\": proba,\n",
    "            \"y_pred\": y_pred,\n",
    "        }\n",
    "    )\n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76209f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "person_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "y_true",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "y_pred_proba",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "y_pred",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "621ec52c-d5c1-4fb7-9338-ad265ae8fb71",
       "rows": [
        [
         "0",
         "2795536102",
         "0",
         "0.0001254021416326357",
         "0"
        ],
        [
         "1",
         "2796668104",
         "0",
         "0.0006086147803528091",
         "0"
        ],
        [
         "2",
         "2815945102",
         "0",
         "0.00019582131977874826",
         "0"
        ],
        [
         "3",
         "2813232102",
         "0",
         "0.0023058245286689746",
         "0"
        ],
        [
         "4",
         "2810968102",
         "0",
         "0.0003525557074554451",
         "0"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred_proba</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2795536102</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2796668104</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2815945102</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2813232102</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2810968102</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    person_id  y_true  y_pred_proba  y_pred\n",
       "0  2795536102       0      0.000125       0\n",
       "1  2796668104       0      0.000609       0\n",
       "2  2815945102       0      0.000196       0\n",
       "3  2813232102       0      0.002306       0\n",
       "4  2810968102       0      0.000353       0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval = make_base_eval_table(X_test, y_test, ids_test, final_lgb)\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1188bca6",
   "metadata": {},
   "source": [
    "### Step 3 — Merge with demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45defbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_eval_and_meta(df_eval, df_meta):\n",
    "    df_merged = df_eval.merge(df_meta, on=\"person_id\", how=\"left\")\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aae0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    person_id  y_true  y_pred_proba  y_pred  age  sex  race_ethnicity  \\\n",
      "0  2795536102       0      0.000125       0   44    1               2   \n",
      "1  2796668104       0      0.000609       0    4    1               1   \n",
      "2  2815945102       0      0.000196       0   60    2               1   \n",
      "3  2813232102       0      0.002306       0   59    1               2   \n",
      "4  2810968102       0      0.000353       0   30    1               2   \n",
      "\n",
      "   hispanic  poverty_category  insurance_coverage  family_income  \\\n",
      "0         2                 4                   1         142202   \n",
      "1         1                 1                   2              0   \n",
      "2         1                 4                   3          64010   \n",
      "3         2                 5                   1         335489   \n",
      "4         2                 4                   3          47840   \n",
      "\n",
      "   self_rated_health  self_rated_mental_health  \n",
      "0                2.0                       2.0  \n",
      "1                4.0                       3.0  \n",
      "2                4.0                       3.0  \n",
      "3                2.0                       2.0  \n",
      "4                1.0                       2.0  \n"
     ]
    }
   ],
   "source": [
    "# merge model outputs with demographics\n",
    "df_fair = merge_eval_and_meta(df_eval, df_meta)\n",
    "print(df_fair.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db8429",
   "metadata": {},
   "source": [
    "### Step 4: Recoding demographic columns with human-readable labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5f7ce",
   "metadata": {},
   "source": [
    "The demographic variables in MEPS are all encoded as numbers, so before I can compute fairness metrics or show any results in the playground, I need them in human-readable form. This step converts all the MEPS-coded fields — sex, race/ethnicity, Hispanic status, poverty category, insurance coverage, and self-rated health — into clear labels. This makes the fairness results interpretable and avoids exposing raw codes in the playground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "249722e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode sex\n",
    "def recode_sex(df):\n",
    "    sex_map = {\n",
    "        1: \"Male\",\n",
    "        2: \"Female\",\n",
    "    }\n",
    "    df[\"sex\"] = df[\"sex\"].map(sex_map)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1a080cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode Race/Ethnicity (RACETHX)\n",
    "def recode_race_ethnicity(df):\n",
    "    race_map = {\n",
    "        1: \"Hispanic\",\n",
    "        2: \"White\",\n",
    "        3: \"Black\",\n",
    "        4: \"Asian\",\n",
    "        5: \"Other OR Multiple\",\n",
    "    }\n",
    "    df[\"race_ethnicity\"] = df[\"race_ethnicity\"].map(race_map)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14668a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode Hispanic Flag (HISPANX)\n",
    "def recode_hispanic(df):\n",
    "    hisp_map = {\n",
    "        1: \"Hispanic\",\n",
    "        2: \"Not Hispanic\",\n",
    "    }\n",
    "    df[\"hispanic\"] = df[\"hispanic\"].map(hisp_map)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a86709d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poverty Category (POVCAT23)\n",
    "def recode_poverty(df):\n",
    "    pov_map = {\n",
    "        1: \"Poor OR negative\",\n",
    "        2: \"low income\",\n",
    "        3: \"Middle income\",\n",
    "        4: \"High income\",\n",
    "        5: \"Unclassifiable\",\n",
    "    }\n",
    "    df[\"poverty_category\"] = df[\"poverty_category\"].map(pov_map)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ad2c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insurance Coverage (INSCOV23)\n",
    "def recode_insurance(df):\n",
    "    ins_map = {\n",
    "        1: \"Any private\",\n",
    "        2: \"Public only\",\n",
    "        3: \"Uninsured\",\n",
    "    }\n",
    "    df[\"insurance_coverage\"] = df[\"insurance_coverage\"].map(ins_map)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "681e99b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode Self-Rated Health (RTHLTH53)\n",
    "def recode_self_rated_health(df):\n",
    "    health_map = {\n",
    "        1: \"Excellent\",\n",
    "        2: \"Very good\",\n",
    "        3: \"Good\",\n",
    "        4: \"Fair\",\n",
    "        5: \"Poor\",\n",
    "    }\n",
    "\n",
    "    df[\"self_rated_health\"] = (\n",
    "        df[\"self_rated_health\"].round().astype(\"Int64\").map(health_map)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f53262ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode Self-Rated Mental Health (MNHLTH53)\n",
    "def recode_self_rated_mental(df):\n",
    "    mental_map = {\n",
    "        1: \"Excellent\",\n",
    "        2: \"Very good\",\n",
    "        3: \"Good\",\n",
    "        4: \"Fair\",\n",
    "        5: \"Poor\",\n",
    "    }\n",
    "\n",
    "    df[\"self_rated_mental_health\"] = (\n",
    "        df[\"self_rated_mental_health\"].round().astype(\"Int64\").map(mental_map)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4fd9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all recodings\n",
    "def apply_all_recodings(df):\n",
    "    return (\n",
    "        df.pipe(recode_sex)\n",
    "        .pipe(recode_race_ethnicity)\n",
    "        .pipe(recode_hispanic)\n",
    "        .pipe(recode_poverty)\n",
    "        .pipe(recode_insurance)\n",
    "        .pipe(recode_self_rated_health)\n",
    "        .pipe(recode_self_rated_mental)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2dfb40e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    person_id  y_true  y_pred_proba  y_pred  age     sex race_ethnicity  \\\n",
      "0  2795536102       0      0.000125       0   44    Male          White   \n",
      "1  2796668104       0      0.000609       0    4    Male       Hispanic   \n",
      "2  2815945102       0      0.000196       0   60  Female       Hispanic   \n",
      "3  2813232102       0      0.002306       0   59    Male          White   \n",
      "4  2810968102       0      0.000353       0   30    Male          White   \n",
      "\n",
      "       hispanic  poverty_category insurance_coverage  family_income  \\\n",
      "0  Not Hispanic       High income        Any private         142202   \n",
      "1      Hispanic  Poor OR negative        Public only              0   \n",
      "2      Hispanic       High income          Uninsured          64010   \n",
      "3  Not Hispanic    Unclassifiable        Any private         335489   \n",
      "4  Not Hispanic       High income          Uninsured          47840   \n",
      "\n",
      "  self_rated_health self_rated_mental_health  \n",
      "0         Very good                Very good  \n",
      "1              Fair                     Good  \n",
      "2              Fair                     Good  \n",
      "3         Very good                Very good  \n",
      "4         Excellent                Very good  \n"
     ]
    }
   ],
   "source": [
    "df = apply_all_recodings(df_fair)\n",
    "print(df_fair.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e464629d",
   "metadata": {},
   "source": [
    "### Step 5: Group level fairness Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54b979",
   "metadata": {},
   "source": [
    "Now that the fairness dataset is fully assembled, the next step is to compute standard group-level performance metrics. This helps quantify how the model behaves for different demographic groups. For example, I can compare recall, false negative rate, or positive prediction rate across race, sex, or income levels. These metrics form the backbone of the fairness playground and make it easy to see where the model treats groups differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aaa96c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def compute_group_metrics(df, group_col):\n",
    "    \"\"\"\n",
    "    Compute standard model performance metrics for each demographic group.\n",
    "    These metrics feed the fairness playground (tables + plots).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for group, g in df.groupby(group_col):\n",
    "        y_true = g[\"y_true\"]\n",
    "        y_pred = g[\"y_pred\"]\n",
    "\n",
    "        # Core metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        # Derived metrics\n",
    "        fnr = 1 - rec\n",
    "        fpr = ((y_pred.eq(1) & y_true.eq(0)).sum()) / max((y_true == 0).sum(), 1)\n",
    "        pos_rate = y_pred.mean()\n",
    "\n",
    "        avg_proba = g[\"y_pred_proba\"].mean()\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                group_col: group,\n",
    "                \"accuracy\": acc,\n",
    "                \"precision\": prec,\n",
    "                \"recall\": rec,\n",
    "                \"false_negative_rate\": fnr,\n",
    "                \"false_positive_rate\": fpr,\n",
    "                \"positive_prediction_rate\": pos_rate,\n",
    "                \"avg_pred_probability\": avg_proba,\n",
    "                \"count\": len(g),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "442faf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_metrics = compute_group_metrics(df_fair, \"race_ethnicity\")\n",
    "sex_metrics = compute_group_metrics(df_fair, \"sex\")\n",
    "poverty_metrics = compute_group_metrics(df_fair, \"poverty_category\")\n",
    "insurance_metrics = compute_group_metrics(df_fair, \"insurance_coverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ebaf6",
   "metadata": {},
   "source": [
    "### Step 6 - Disparity Gap Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb88da5",
   "metadata": {},
   "source": [
    "Group metrics tell me how the model performs within each demographic group, but they don’t directly show how far apart the groups are. In this step, I compute simple disparity metrics by picking a reference group and measuring gaps in key quantities like positive prediction rate, recall, and false negative rate. These gaps are what the fairness playground will display when highlighting where the model is more or less sensitive for different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72931389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_disparities(group_df, group_col, reference_group=None):\n",
    "    \"\"\"\n",
    "    Take a group metrics table (from compute_group_metrics)\n",
    "    and compute gaps vs a reference group.\n",
    "\n",
    "    If reference_group is None, uses the group with the largest count.\n",
    "    \"\"\"\n",
    "    df = group_df.copy()\n",
    "\n",
    "    # Choose reference\n",
    "    if reference_group is None:\n",
    "        reference_group = df.sort_values(\"count\", ascending=False)[group_col].iloc[0]\n",
    "\n",
    "    ref_row = df[df[group_col] == reference_group].iloc[0]\n",
    "\n",
    "    # Metrics we want gaps for\n",
    "    gap_metrics = [\n",
    "        \"positive_prediction_rate\",\n",
    "        \"recall\",\n",
    "        \"precision\",\n",
    "        \"false_negative_rate\",\n",
    "        \"false_positive_rate\",\n",
    "    ]\n",
    "\n",
    "    for m in gap_metrics:\n",
    "        gap_col = f\"{m}_gap_vs_ref\"\n",
    "        df[gap_col] = df[m] - ref_row[m]\n",
    "\n",
    "    df[\"reference_group\"] = reference_group\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce917d",
   "metadata": {},
   "source": [
    "### Step 7 -The Explainaibility Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72cb9cc",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e01d46b",
   "metadata": {},
   "source": [
    "#### 7A. Global Explanations (Model-Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb1a16",
   "metadata": {},
   "source": [
    "To make the model’s behavior more interpretable, I compute global SHAP values to understand which features drive predictions overall. This gives me a ranked list of factors that the model relies on the most, which will be displayed in the playground to give users a simple sense of what the model cares about at a high level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7f0ab",
   "metadata": {},
   "source": [
    "##### Global SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f92b891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_global_shap(model, X_test):\n",
    "    \"\"\"\n",
    "    Compute SHAP values for the LightGBM model on the test set.\n",
    "    \"\"\"\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)[1]  # class 1 (hospitalized)\n",
    "    return explainer, shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "adc1509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/shap/explainers/_tree.py:586: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "explainer, shap_values_test = compute_global_shap(final_lgb, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1e4c9",
   "metadata": {},
   "source": [
    "##### Rank Global Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f12e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_shap_importance(shap_values, feature_names, top_n=10):\n",
    "    mean_abs = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    df_imp = (\n",
    "        pd.DataFrame({\"feature\": feature_names, \"mean_abs_shap\": mean_abs})\n",
    "        .sort_values(\"mean_abs_shap\", ascending=False)\n",
    "        .head(top_n)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9c88ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "feature",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mean_abs_shap",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "17c9b79a-7981-4f0f-a408-0be20421b86a",
       "rows": [
        [
         "0",
         "age",
         "0.09471282775924247"
        ],
        [
         "1",
         "insurance_category_3",
         "0.09471282775924247"
        ],
        [
         "2",
         "insurance_category_5",
         "0.09471282775924247"
        ],
        [
         "3",
         "insurance_category_6",
         "0.09471282775924247"
        ],
        [
         "4",
         "insurance_category_7",
         "0.09471282775924247"
        ],
        [
         "5",
         "insurance_category_8",
         "0.09471282775924247"
        ],
        [
         "6",
         "region_1",
         "0.09471282775924247"
        ],
        [
         "7",
         "region_2",
         "0.09471282775924247"
        ],
        [
         "8",
         "region_3",
         "0.09471282775924247"
        ],
        [
         "9",
         "region_4",
         "0.09471282775924247"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>mean_abs_shap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>0.094713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>insurance_category_3</td>\n",
       "      <td>0.094713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>insurance_category_5</td>\n",
       "      <td>0.094713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>insurance_category_6</td>\n",
       "      <td>0.094713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insurance_category_7</td>\n",
       "      <td>0.094713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>insurance_category_8</td>\n",
       "      <td>0.094713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>region_1</td>\n",
       "      <td>0.094713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>region_2</td>\n",
       "      <td>0.094713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>region_3</td>\n",
       "      <td>0.094713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>region_4</td>\n",
       "      <td>0.094713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                feature  mean_abs_shap\n",
       "0                   age       0.094713\n",
       "1  insurance_category_3       0.094713\n",
       "2  insurance_category_5       0.094713\n",
       "3  insurance_category_6       0.094713\n",
       "4  insurance_category_7       0.094713\n",
       "5  insurance_category_8       0.094713\n",
       "6              region_1       0.094713\n",
       "7              region_2       0.094713\n",
       "8              region_3       0.094713\n",
       "9              region_4       0.094713"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_importance = global_shap_importance(shap_values_test, X_test.columns)\n",
    "global_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7492f9a3",
   "metadata": {},
   "source": [
    "##### 7B. Group Explanations (Fairness + Explainability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc834ed",
   "metadata": {},
   "source": [
    "To understand whether the model reasons differently for different groups, I compute SHAP feature importance separately within each demographic group. These group-specific explanations highlight whether certain features are more influential for one group than another, which makes it easier to see how model behavior varies across demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "880bdc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_shap_to_df(df_fair, shap_values, feature_names):\n",
    "    shap_df = pd.DataFrame(shap_values, columns=feature_names)\n",
    "    shap_df[\"person_id\"] = df_fair[\"person_id\"].values\n",
    "    return df_fair.merge(shap_df, on=\"person_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c5468d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (65, 1), indices imply (65, 65)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_fair_shap \u001b[38;5;241m=\u001b[39m \u001b[43mattach_shap_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_fair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshap_values_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m, in \u001b[0;36mattach_shap_to_df\u001b[0;34m(df_fair, shap_values, feature_names)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattach_shap_to_df\u001b[39m(df_fair, shap_values, feature_names):\n\u001b[0;32m----> 2\u001b[0m     shap_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     shap_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_fair[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_fair\u001b[38;5;241m.\u001b[39mmerge(shap_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:827\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    817\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    824\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[1;32m    825\u001b[0m         )\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 827\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[1;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[1;32m    334\u001b[0m )\n\u001b[0;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (65, 1), indices imply (65, 65)"
     ]
    }
   ],
   "source": [
    "df_fair_shap = attach_shap_to_df(df_fair, shap_values_test, X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd970679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_shap_importance(df_with_shap, group_col, top_n=10):\n",
    "    exclude = {\n",
    "        \"person_id\",\n",
    "        \"y_true\",\n",
    "        \"y_pred\",\n",
    "        \"y_pred_proba\",\n",
    "        \"sex\",\n",
    "        \"race_ethnicity\",\n",
    "        \"hispanic\",\n",
    "        \"poverty_category\",\n",
    "        \"insurance_coverage\",\n",
    "        \"self_rated_health\",\n",
    "        \"self_rated_mental_health\",\n",
    "        \"age\",\n",
    "        \"age_group\",\n",
    "    }\n",
    "\n",
    "    shap_cols = [c for c in df_with_shap.columns if c not in exclude]\n",
    "\n",
    "    results = {}\n",
    "    for group, g in df_with_shap.groupby(group_col):\n",
    "        avg_abs = g[shap_cols].abs().mean().sort_values(ascending=False)\n",
    "        results[group] = avg_abs.head(top_n)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3960546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_shap = group_shap_importance(df_fair_shap, \"race_ethnicity\")\n",
    "sex_shap = group_shap_importance(df_fair_shap, \"sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e18c7",
   "metadata": {},
   "source": [
    "#### 7C. Local Explanations (Per-Person)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc98e2d",
   "metadata": {},
   "source": [
    "For user-selected individuals, I compute a local SHAP explanation that breaks down which features most increased or decreased that person’s predicted risk. This forms the basis of the personalized “explanation card” in the playground, showing exactly why the model made its prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1401b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_explanation(explainer, model, X_row, top_k=5):\n",
    "    x = X_row.values.reshape(1, -1)\n",
    "\n",
    "    # SHAP\n",
    "    shap_vals = explainer.shap_values(x)[1][0]\n",
    "    base = explainer.expected_value[1]\n",
    "\n",
    "    # Prediction\n",
    "    proba = model.predict_proba(x)[0, 1]\n",
    "\n",
    "    contrib = pd.Series(shap_vals, index=X_row.index)\n",
    "    pos = contrib[contrib > 0].sort_values(ascending=False).head(top_k)\n",
    "    neg = contrib[contrib < 0].sort_values(ascending=True).head(top_k)\n",
    "\n",
    "    return {\n",
    "        \"prediction_proba\": float(proba),\n",
    "        \"base_value\": float(base),\n",
    "        \"top_positive\": list(pos.items()),\n",
    "        \"top_negative\": list(neg.items()),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba6e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = X_test.index[0]\n",
    "row = X_test.loc[idx]\n",
    "local_exp = local_explanation(explainer, final_lgb, row)\n",
    "local_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd71f42",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Step 8 (local explanations)\n",
    "\n",
    "### Step 9 (SHAP integration)\n",
    "\n",
    "### Step 10 (counterfactuals — later)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
