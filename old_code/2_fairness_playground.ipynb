{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a253b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import random\n",
    "import joblib\n",
    "import pathlib\n",
    "import shap\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "# Display options\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926638aa",
   "metadata": {},
   "source": [
    "## Step 1 — Load artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "451d2f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load path and get all the required files\n",
    "def get_paths(base=\"fairness_artifacts\"):\n",
    "    base = pathlib.Path(base)\n",
    "    return {\n",
    "        \"X_train\": base / \"X_train.parquet\",\n",
    "        \"X_test\": base / \"X_test.parquet\",\n",
    "        \"y_train\": base / \"y_train.parquet\",\n",
    "        \"y_test\": base / \"y_test.parquet\",\n",
    "        \"df_meta\": base / \"df_meta.parquet\",\n",
    "        \"ids_test\": base / \"ids_test.parquet\",\n",
    "        \"model\": base / \"final_lightgbm_model.pkl\",\n",
    "    }\n",
    "\n",
    "\n",
    "def load_artifacts(base=\"fairness_artifacts\"):\n",
    "    paths = get_paths(base)\n",
    "\n",
    "    X_train = pd.read_parquet(paths[\"X_train\"])\n",
    "    X_test = pd.read_parquet(paths[\"X_test\"])\n",
    "    y_train = pd.read_parquet(paths[\"y_train\"])[\"hospitalized\"]\n",
    "    y_test = pd.read_parquet(paths[\"y_test\"])[\"hospitalized\"]\n",
    "    df_meta = pd.read_parquet(paths[\"df_meta\"])\n",
    "    ids_test = pd.read_parquet(paths[\"ids_test\"])[\"person_id\"]\n",
    "    model = joblib.load(paths[\"model\"])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, df_meta, ids_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d6c59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, df_meta, ids_test, final_lgb = load_artifacts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7308add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      "X Train: (15135, 65)\n",
      "X Test: (3784, 65)\n",
      "y Train: (15135,)\n",
      "y Test: (3784,)\n",
      "ids_test: (3784,)\n",
      "df_meta: (18919, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\")\n",
    "print(\"X Train:\", X_train.shape)\n",
    "print(\"X Test:\", X_test.shape)\n",
    "print(\"y Train:\", y_train.shape)\n",
    "print(\"y Test:\", y_test.shape)\n",
    "print(\"ids_test:\", ids_test.shape)\n",
    "print(\"df_meta:\", df_meta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423ea49d",
   "metadata": {},
   "source": [
    "## Step 2 — Build base evaluation table\n",
    "\n",
    "For the fairness playground, I need a clean table that captures what the model actually did on the test set. Thus, I will now build a base evaluation table on the test set that includes person_id, the true label, the predicted probability, and the final 0/1 prediction. This is the core model-output layer I’ll use when joining with demographics and computing group-wise fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd18a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_base_eval_table(X_test, y_test, ids_test, model, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Build a base evaluation table on the test set.\n",
    "    \"\"\"\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (proba >= threshold).astype(int)\n",
    "\n",
    "    df_eval = pd.DataFrame(\n",
    "        {\n",
    "            \"person_id\": ids_test.values,\n",
    "            \"y_true\": y_test.values,\n",
    "            \"y_pred_proba\": proba,\n",
    "            \"y_pred\": y_pred,\n",
    "        }\n",
    "    )\n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "786bf732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "person_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "y_true",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "y_pred_proba",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "y_pred",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "08b92127-2643-4be1-acba-5d3f05c7a863",
       "rows": [
        [
         "0",
         "2795536102",
         "0",
         "0.0001254021416326357",
         "0"
        ],
        [
         "1",
         "2796668104",
         "0",
         "0.0006086147803528091",
         "0"
        ],
        [
         "2",
         "2815945102",
         "0",
         "0.00019582131977874826",
         "0"
        ],
        [
         "3",
         "2813232102",
         "0",
         "0.0023058245286689746",
         "0"
        ],
        [
         "4",
         "2810968102",
         "0",
         "0.0003525557074554451",
         "0"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred_proba</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2795536102</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2796668104</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2815945102</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2813232102</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2810968102</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    person_id  y_true  y_pred_proba  y_pred\n",
       "0  2795536102       0      0.000125       0\n",
       "1  2796668104       0      0.000609       0\n",
       "2  2815945102       0      0.000196       0\n",
       "3  2813232102       0      0.002306       0\n",
       "4  2810968102       0      0.000353       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_eval = make_base_eval_table(X_test, y_test, ids_test, final_lgb)\n",
    "display(df_eval.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c5876",
   "metadata": {},
   "source": [
    "### Step 3 — Merge with demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc75094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_eval_and_meta(df_eval, df_meta):\n",
    "    df_merged = df_eval.merge(df_meta, on=\"person_id\", how=\"left\")\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bebe44a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    person_id  y_true  y_pred_proba  y_pred  age  sex  race_ethnicity  \\\n",
      "0  2795536102       0      0.000125       0   44    1               2   \n",
      "1  2796668104       0      0.000609       0    4    1               1   \n",
      "2  2815945102       0      0.000196       0   60    2               1   \n",
      "3  2813232102       0      0.002306       0   59    1               2   \n",
      "4  2810968102       0      0.000353       0   30    1               2   \n",
      "\n",
      "   hispanic  poverty_category  insurance_coverage  family_income  \\\n",
      "0         2                 4                   1         142202   \n",
      "1         1                 1                   2              0   \n",
      "2         1                 4                   3          64010   \n",
      "3         2                 5                   1         335489   \n",
      "4         2                 4                   3          47840   \n",
      "\n",
      "   self_rated_health  self_rated_mental_health  \n",
      "0                2.0                       2.0  \n",
      "1                4.0                       3.0  \n",
      "2                4.0                       3.0  \n",
      "3                2.0                       2.0  \n",
      "4                1.0                       2.0  \n"
     ]
    }
   ],
   "source": [
    "# merge model outputs with demographics\n",
    "df_fair = merge_eval_and_meta(df_eval, df_meta)\n",
    "print(df_fair.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6bc23e",
   "metadata": {},
   "source": [
    "### Step 4: Recoding demographic columns with human-readable labels\n",
    "\n",
    "The demographic variables in MEPS are all encoded as numbers, so before I can compute fairness metrics or show any results in the playground, I need them in human-readable form. This step converts all the MEPS-coded fields — sex, race/ethnicity, Hispanic status, poverty category, insurance coverage, and self-rated health — into clear labels. This makes the fairness results interpretable and avoids exposing raw codes in the playground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16b57109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode sex\n",
    "def recode_sex(df):\n",
    "    sex_map = {1: \"Male\", 2: \"Female\"}\n",
    "    df[\"sex\"] = df[\"sex\"].map(sex_map)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Recode Race/Ethnicity (RACETHX)\n",
    "def recode_race_ethnicity(df):\n",
    "    race_map = {\n",
    "        1: \"Hispanic\",\n",
    "        2: \"White\",\n",
    "        3: \"Black\",\n",
    "        4: \"Asian\",\n",
    "        5: \"Other OR Multiple\",\n",
    "    }\n",
    "    df[\"race_ethnicity\"] = df[\"race_ethnicity\"].map(race_map)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Recode Hispanic Flag (HISPANX)\n",
    "def recode_hispanic(df):\n",
    "    hisp_map = {1: \"Hispanic\", 2: \"Not Hispanic\"}\n",
    "    df[\"hispanic\"] = df[\"hispanic\"].map(hisp_map)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Poverty Category (POVCAT23)\n",
    "def recode_poverty(df):\n",
    "    pov_map = {\n",
    "        1: \"Poor OR negative\",\n",
    "        2: \"Low income\",\n",
    "        3: \"Middle income\",\n",
    "        4: \"High income\",\n",
    "        5: \"Unclassifiable\",\n",
    "    }\n",
    "    df[\"poverty_category\"] = df[\"poverty_category\"].map(pov_map)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Insurance Coverage (INSCOV23)\n",
    "def recode_insurance(df):\n",
    "    ins_map = {1: \"Any private\", 2: \"Public only\", 3: \"Uninsured\"}\n",
    "    df[\"insurance_coverage\"] = df[\"insurance_coverage\"].map(ins_map)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Recode Self-Rated Health (RTHLTH53)\n",
    "def recode_self_rated_health(df):\n",
    "    health_map = {\n",
    "        1: \"Excellent\",\n",
    "        2: \"Very good\",\n",
    "        3: \"Good\",\n",
    "        4: \"Fair\",\n",
    "        5: \"Poor\",\n",
    "    }\n",
    "    df[\"self_rated_health\"] = (\n",
    "        df[\"self_rated_health\"].round().astype(\"Int64\").map(health_map)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# Recode Self-Rated Mental Health (MNHLTH53)\n",
    "def recode_self_rated_mental(df):\n",
    "    mental_map = {\n",
    "        1: \"Excellent\",\n",
    "        2: \"Very good\",\n",
    "        3: \"Good\",\n",
    "        4: \"Fair\",\n",
    "        5: \"Poor\",\n",
    "    }\n",
    "    df[\"self_rated_mental_health\"] = (\n",
    "        df[\"self_rated_mental_health\"].round().astype(\"Int64\").map(mental_map)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply all recodings\n",
    "def apply_all_recodings(df):\n",
    "    return (\n",
    "        df.pipe(recode_sex)\n",
    "        .pipe(recode_race_ethnicity)\n",
    "        .pipe(recode_hispanic)\n",
    "        .pipe(recode_poverty)\n",
    "        .pipe(recode_insurance)\n",
    "        .pipe(recode_self_rated_health)\n",
    "        .pipe(recode_self_rated_mental)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb4ee332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    person_id  y_true  y_pred_proba  y_pred  age     sex race_ethnicity  \\\n",
      "0  2795536102       0      0.000125       0   44    Male          White   \n",
      "1  2796668104       0      0.000609       0    4    Male       Hispanic   \n",
      "2  2815945102       0      0.000196       0   60  Female       Hispanic   \n",
      "3  2813232102       0      0.002306       0   59    Male          White   \n",
      "4  2810968102       0      0.000353       0   30    Male          White   \n",
      "\n",
      "       hispanic  poverty_category insurance_coverage  family_income  \\\n",
      "0  Not Hispanic       High income        Any private         142202   \n",
      "1      Hispanic  Poor OR negative        Public only              0   \n",
      "2      Hispanic       High income          Uninsured          64010   \n",
      "3  Not Hispanic    Unclassifiable        Any private         335489   \n",
      "4  Not Hispanic       High income          Uninsured          47840   \n",
      "\n",
      "  self_rated_health self_rated_mental_health  \n",
      "0         Very good                Very good  \n",
      "1              Fair                     Good  \n",
      "2              Fair                     Good  \n",
      "3         Very good                Very good  \n",
      "4         Excellent                Very good  \n"
     ]
    }
   ],
   "source": [
    "df_fair = apply_all_recodings(df_fair)\n",
    "print(df_fair.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bfa1b1",
   "metadata": {},
   "source": [
    "### Step 5: Group level fairness Metrics\n",
    "\n",
    "Now that the fairness dataset is fully assembled, the next step is to compute standard group-level performance metrics. This helps quantify how the model behaves for different demographic groups. For example, I can compare recall, false negative rate, or positive prediction rate across race, sex, or income levels. These metrics form the backbone of the fairness playground and make it easy to see where the model treats groups differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04400f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_group_metrics(df, group_col):\n",
    "    \"\"\"\n",
    "    Compute standard model performance metrics for each demographic group.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for group, g in df.groupby(group_col):\n",
    "        y_true = g[\"y_true\"]\n",
    "        y_pred = g[\"y_pred\"]\n",
    "\n",
    "        # Core metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        # Derived metrics\n",
    "        fnr = 1 - rec\n",
    "        fpr = ((y_pred.eq(1) & y_true.eq(0)).sum()) / max((y_true == 0).sum(), 1)\n",
    "        pos_rate = y_pred.mean()\n",
    "        avg_proba = g[\"y_pred_proba\"].mean()\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                group_col: group,\n",
    "                \"accuracy\": acc,\n",
    "                \"precision\": prec,\n",
    "                \"recall\": rec,\n",
    "                \"false_negative_rate\": fnr,\n",
    "                \"false_positive_rate\": fpr,\n",
    "                \"positive_prediction_rate\": pos_rate,\n",
    "                \"avg_pred_probability\": avg_proba,\n",
    "                \"count\": len(g),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f478996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for each demographic group\n",
    "race_metrics = compute_group_metrics(df_fair, \"race_ethnicity\")\n",
    "sex_metrics = compute_group_metrics(df_fair, \"sex\")\n",
    "poverty_metrics = compute_group_metrics(df_fair, \"poverty_category\")\n",
    "insurance_metrics = compute_group_metrics(df_fair, \"insurance_coverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285268eb",
   "metadata": {},
   "source": [
    "### Step 6 - Disparity Gap Metrics\n",
    "\n",
    "Group metrics tell me how the model performs within each demographic group, but they don’t directly show how far apart the groups are. In this step, I compute simple disparity metrics by picking a reference group and measuring gaps in key quantities like positive prediction rate, recall, and false negative rate. These gaps are what the fairness playground will display when highlighting where the model is more or less sensitive for different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc1f8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_disparities(group_df, group_col, reference_group=None):\n",
    "    \"\"\"\n",
    "    Compute gaps in metrics versus a reference group.\n",
    "    If reference_group is None, uses the group with the largest count.\n",
    "    \"\"\"\n",
    "    df = group_df.copy()\n",
    "\n",
    "    # Choose reference\n",
    "    if reference_group is None:\n",
    "        reference_group = df.sort_values(\"count\", ascending=False)[group_col].iloc[0]\n",
    "\n",
    "    ref_row = df[df[group_col] == reference_group].iloc[0]\n",
    "\n",
    "    # Metrics we want gaps for\n",
    "    gap_metrics = [\n",
    "        \"positive_prediction_rate\",\n",
    "        \"recall\",\n",
    "        \"precision\",\n",
    "        \"false_negative_rate\",\n",
    "        \"false_positive_rate\",\n",
    "    ]\n",
    "\n",
    "    for m in gap_metrics:\n",
    "        gap_col = f\"{m}_gap_vs_ref\"\n",
    "        df[gap_col] = df[m] - ref_row[m]\n",
    "\n",
    "    df[\"reference_group\"] = reference_group\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cdf1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute disparities (gaps) for each demographic\n",
    "race_disparities = compute_disparities(race_metrics, \"race_ethnicity\")\n",
    "sex_disparities = compute_disparities(sex_metrics, \"sex\")\n",
    "poverty_disparities = compute_disparities(poverty_metrics, \"poverty_category\")\n",
    "insurance_disparities = compute_disparities(insurance_metrics, \"insurance_coverage\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
